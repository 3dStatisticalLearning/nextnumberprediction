{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcf209b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STARTING TRAINING\n",
      "======================================================================\n",
      "\n",
      "[1/6] Loading dataset...\n",
      "✓ Dataset loaded: (7199, 20) (rows × columns)\n",
      "  Value range: [1, 70]\n",
      "  Unique values: 70\n",
      "✓ Value mappings saved\n",
      "\n",
      "[2/6] Converting data to model format...\n",
      "✓ Data converted to indices (range: 0-69)\n",
      "✓ Statistics saved\n",
      "\n",
      "[3/6] Preparing training and validation sets...\n",
      "✓ Training samples: 6110\n",
      "✓ Validation samples: 1079\n",
      "\n",
      "[4/6] Initializing model...\n",
      "✓ Using device: cpu\n",
      "✓ Model created with 1,984,632 parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[5/6] Starting training...\n",
      "======================================================================\n",
      "Epoch   1/200 | Loss: 4.3183/4.2799 | Acc: 1.44%/1.41%\n",
      "  ✓ Model saved! Val Acc: 1.41%\n",
      "  ✓ Model saved! Val Acc: 1.52%\n",
      "  ✓ Model saved! Val Acc: 1.40%\n",
      "  ✓ Model saved! Val Acc: 1.48%\n",
      "Epoch   5/200 | Loss: 4.2515/4.2538 | Acc: 1.58%/1.39%\n",
      "  ✓ Model saved! Val Acc: 1.39%\n",
      "  ✓ Model saved! Val Acc: 1.55%\n",
      "Epoch  10/200 | Loss: 4.2467/4.2525 | Acc: 1.65%/1.64%\n",
      "  ✓ Model saved! Val Acc: 1.46%\n",
      "Epoch  15/200 | Loss: 4.2458/4.2531 | Acc: 1.68%/1.49%\n",
      "Epoch  20/200 | Loss: 4.2442/4.2530 | Acc: 1.72%/1.44%\n",
      "Epoch  25/200 | Loss: 4.2442/4.2528 | Acc: 1.77%/1.53%\n",
      "Epoch  30/200 | Loss: 4.2436/4.2530 | Acc: 1.80%/1.57%\n",
      "Epoch  35/200 | Loss: 4.2436/4.2534 | Acc: 1.77%/1.53%\n",
      "\n",
      "Early stopping at epoch 36\n",
      "\n",
      "======================================================================\n",
      "[6/6] Training completed!\n",
      "✓ Best validation accuracy: 1.46%\n",
      "✓ Best validation loss: 4.2518\n",
      "======================================================================\n",
      "\n",
      "✓ All files saved:\n",
      "  - model.pth\n",
      "  - model_stats.json\n",
      "  - model_config.json\n",
      "  - value_mappings.npy\n",
      "\n",
      "Ready to run: python app.py\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Configuration - SIMPLIFIED for better learning\n",
    "CONFIG = {\n",
    "    \"data_path\": r\"C:\\Users\\Admin\\Desktop\\3dstatisticallearning\\Use Cases\\Use Cases Machine Learning\\Sebastian_Zahlenreihe\\Predict Next Number\\Predict Next Number\\Mappe_nagelneu_new.xlsx\",\n",
    "    \"sequence_length\": 10,  \n",
    "    \"hidden_dim\": 256,  # Reduced\n",
    "    \"num_heads\": 8,\n",
    "    \"num_layers\": 3,  # Reduced\n",
    "    \"dropout\": 0.2,\n",
    "    \"learning_rate\": 0.001,  # INCREASED - was too low!\n",
    "    \"batch_size\": 32,  # Increased\n",
    "    \"epochs\": 200,\n",
    "    \"patience\": 25,\n",
    "}\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n[1/6] Loading dataset...\")\n",
    "data = pd.read_excel(CONFIG[\"data_path\"], header=None)\n",
    "dataset = data.values.astype(np.int64)\n",
    "\n",
    "print(f\"✓ Dataset loaded: {dataset.shape} (rows × columns)\")\n",
    "print(f\"  Value range: [{dataset.min()}, {dataset.max()}]\")\n",
    "\n",
    "# Get unique values\n",
    "unique_values = np.unique(dataset)\n",
    "num_classes = len(unique_values)\n",
    "value_to_idx = {int(val): idx for idx, val in enumerate(unique_values)}\n",
    "idx_to_value = {idx: int(val) for idx, val in enumerate(unique_values)}\n",
    "\n",
    "print(f\"  Unique values: {num_classes}\")\n",
    "\n",
    "# Save mappings immediately\n",
    "np.save(\"value_mappings.npy\", unique_values)\n",
    "print(\"✓ Value mappings saved\")\n",
    "\n",
    "# Convert dataset to indices\n",
    "print(\"\\n[2/6] Converting data to model format...\")\n",
    "dataset_indices = np.zeros_like(dataset, dtype=np.int64)\n",
    "for i in range(dataset.shape[0]):\n",
    "    for j in range(dataset.shape[1]):\n",
    "        dataset_indices[i, j] = value_to_idx[int(dataset[i, j])]\n",
    "\n",
    "print(f\"✓ Data converted to indices (range: 0-{num_classes-1})\")\n",
    "\n",
    "# Save statistics\n",
    "stats = {\n",
    "    \"min\": int(dataset.min()),\n",
    "    \"max\": int(dataset.max()),\n",
    "    \"mean\": float(dataset.mean()),\n",
    "    \"std\": float(dataset.std()),\n",
    "    \"num_columns\": int(dataset.shape[1]),\n",
    "    \"num_classes\": int(num_classes),\n",
    "    \"unique_values\": unique_values.tolist()\n",
    "}\n",
    "\n",
    "with open(\"model_stats.json\", \"w\") as f:\n",
    "    json.dump(stats, f, indent=2)\n",
    "print(\"✓ Statistics saved\")\n",
    "\n",
    "# Dataset class\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, data, sequence_length):\n",
    "        self.data = data\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.sequence_length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx + self.sequence_length]\n",
    "        y = self.data[idx + self.sequence_length]\n",
    "        return torch.LongTensor(x), torch.LongTensor(y)\n",
    "\n",
    "# Simplified Model\n",
    "class NumberPredictor(nn.Module):\n",
    "    def __init__(self, num_classes, num_positions, hidden_dim, num_heads, num_layers, dropout=0.2):\n",
    "        super(NumberPredictor, self).__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.num_positions = num_positions\n",
    "        \n",
    "        # Embeddings\n",
    "        self.value_embedding = nn.Embedding(num_classes, hidden_dim)\n",
    "        self.position_embedding = nn.Embedding(100, hidden_dim)\n",
    "        \n",
    "        # Transformer\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=hidden_dim * 2,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        \n",
    "        # Output heads - one per position\n",
    "        self.output_heads = nn.ModuleList([\n",
    "            nn.Linear(hidden_dim, num_classes) for _ in range(num_positions)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, num_positions)\n",
    "        batch_size, seq_len, num_pos = x.shape\n",
    "        \n",
    "        # Embed each value\n",
    "        x_flat = x.reshape(batch_size * seq_len * num_pos)\n",
    "        embedded = self.value_embedding(x_flat)\n",
    "        embedded = embedded.reshape(batch_size, seq_len, num_pos, -1)\n",
    "        \n",
    "        # Average across positions to get sequence representation\n",
    "        x = embedded.mean(dim=2)  # (batch, seq_len, hidden_dim)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        positions = torch.arange(seq_len, device=x.device)\n",
    "        pos_emb = self.position_embedding(positions).unsqueeze(0)\n",
    "        x = x + pos_emb\n",
    "        \n",
    "        # Transformer\n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        # Use last timestep\n",
    "        x = x[:, -1, :]  # (batch, hidden_dim)\n",
    "        \n",
    "        # Predict each position\n",
    "        outputs = [head(x) for head in self.output_heads]\n",
    "        return torch.stack(outputs, dim=1)  # (batch, num_positions, num_classes)\n",
    "\n",
    "# Prepare data\n",
    "print(\"\\n[3/6] Preparing training and validation sets...\")\n",
    "sequence_length = CONFIG[\"sequence_length\"]\n",
    "dataset_obj = SequenceDataset(dataset_indices, sequence_length)\n",
    "\n",
    "train_size = int(0.85 * len(dataset_obj))\n",
    "val_size = len(dataset_obj) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    dataset_obj, [train_size, val_size]\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=False)\n",
    "\n",
    "print(f\"✓ Training samples: {len(train_dataset)}\")\n",
    "print(f\"✓ Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "# Initialize model\n",
    "print(\"\\n[4/6] Initializing model...\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"✓ Using device: {device}\")\n",
    "\n",
    "model = NumberPredictor(\n",
    "    num_classes=num_classes,\n",
    "    num_positions=dataset.shape[1],\n",
    "    hidden_dim=CONFIG[\"hidden_dim\"],\n",
    "    num_heads=CONFIG[\"num_heads\"],\n",
    "    num_layers=CONFIG[\"num_layers\"],\n",
    "    dropout=CONFIG[\"dropout\"]\n",
    ").to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"✓ Model created with {total_params:,} parameters\")\n",
    "\n",
    "# Training setup\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=CONFIG[\"learning_rate\"])\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, verbose=True)\n",
    "\n",
    "# Accuracy calculation\n",
    "def calculate_accuracy(outputs, targets):\n",
    "    predictions = torch.argmax(outputs, dim=2)\n",
    "    correct = (predictions == targets).float()\n",
    "    return correct.mean().item() * 100\n",
    "\n",
    "# Training loop\n",
    "print(\"\\n[5/6] Starting training...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_val_acc = 0.0\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(CONFIG[\"epochs\"]):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    \n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "        \n",
    "        # Calculate loss for all positions\n",
    "        loss = 0\n",
    "        for i in range(outputs.shape[1]):\n",
    "            loss += criterion(outputs[:, i, :], batch_y[:, i])\n",
    "        loss = loss / outputs.shape[1]\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        train_acc += calculate_accuracy(outputs, batch_y)\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    train_acc /= len(train_loader)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_acc = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in val_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_x)\n",
    "            \n",
    "            loss = 0\n",
    "            for i in range(outputs.shape[1]):\n",
    "                loss += criterion(outputs[:, i, :], batch_y[:, i])\n",
    "            loss = loss / outputs.shape[1]\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            val_acc += calculate_accuracy(outputs, batch_y)\n",
    "    \n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc /= len(val_loader)\n",
    "    \n",
    "    # Scheduler step\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Print progress\n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1:3d}/{CONFIG['epochs']} | \"\n",
    "              f\"Loss: {train_loss:.4f}/{val_loss:.4f} | \"\n",
    "              f\"Acc: {train_acc:.2f}%/{val_acc:.2f}%\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_val_acc = val_acc\n",
    "        patience_counter = 0\n",
    "        \n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'val_acc': val_acc,\n",
    "        }, \"model.pth\")\n",
    "        \n",
    "        print(f\"  ✓ Model saved! Val Acc: {val_acc:.2f}%\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= CONFIG[\"patience\"]:\n",
    "            print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"[6/6] Training completed!\")\n",
    "print(f\"✓ Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "print(f\"✓ Best validation loss: {best_val_loss:.4f}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Save config\n",
    "CONFIG[\"best_val_acc\"] = best_val_acc\n",
    "CONFIG[\"best_val_loss\"] = best_val_loss\n",
    "with open(\"model_config.json\", \"w\") as f:\n",
    "    json.dump(CONFIG, f, indent=2)\n",
    "\n",
    "print(\"\\n✓ All files saved:\")\n",
    "print(\"  - model.pth\")\n",
    "print(\"  - model_stats.json\")\n",
    "print(\"  - model_config.json\")\n",
    "print(\"  - value_mappings.npy\")\n",
    "print(\"\\nReady to run: python app.py\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1caba2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
